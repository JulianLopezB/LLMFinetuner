{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab-badge",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/your-notebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-packages",
   "metadata": {},
   "source": [
    "## Install Necessary Packages\n",
    "First, we need to install the required packages for our fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assuming the repository is already cloned and environment is set up.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Google Colab\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !git clone https://github.com/JulianLopezB/LLMFinetuner.git\n",
    "    !python LLMFinetuner/setup_environment.py\n",
    "    # Add the cloned repository to the Python path\n",
    "    sys.path.append('/content/LLMFinetuner')\n",
    "else:\n",
    "    print(\"Assuming the repository is already cloned and environment is set up.\")\n",
    "    # Add the parent directory to the Python path\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-libraries",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries\n",
    "Next, we import all the necessary libraries and modules that we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup-env",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/Work/Stefanini/LLMFinetuner/llmfinetuner/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from omegaconf import OmegaConf\n",
    "from src import DataLoader, ModelSetup, CustomTrainer, Evaluator, HuggingFaceIntegration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check-environment",
   "metadata": {},
   "source": [
    "## Check Environment\n",
    "Ensure that CUDA is available and the necessary environment variables are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "check-env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Please check your installation of CUDA and NVIDIA drivers.\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"CUDA is not available. Please check your installation of CUDA and NVIDIA drivers.\")\n",
    "\n",
    "# Check for HUGGINGFACE_TOKEN environment variable\n",
    "if 'HUGGINGFACE_TOKEN' not in os.environ:\n",
    "    print(\"HUGGINGFACE_TOKEN is not set. Please set this environment variable.\")\n",
    "    from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain-config",
   "metadata": {},
   "source": [
    "## Configuration YAML Explanation\n",
    "The configuration YAML file consists of several sections that define the parameters for the model, dataset, and training process. Here is a breakdown of the sections:\n",
    "\n",
    "- **model**: Contains the model name, new model name, and quantization settings.\n",
    "  - `name`: The name of the pre-trained model to use.\n",
    "  - `new_model`: The name to save the fine-tuned model.\n",
    "  - `quantization`: Settings for model quantization.\n",
    "  - `device_map`: Device mapping for model loading.\n",
    "- **dataset**: Contains the dataset path and type.\n",
    "  - `path`: The path to the dataset file.\n",
    "  - `type`: The type of dataset.\n",
    "  - `from_huggingface`: Boolean indicating if the dataset is from Hugging Face.\n",
    "- **training**: Contains training parameters and settings.\n",
    "  - `output_dir`: Directory to save the output.\n",
    "  - `peft_enabled`: Boolean indicating if PEFT is enabled.\n",
    "  - `lora_config`: Configuration for PEFT.\n",
    "  - `hf_push`: Boolean indicating if the model should be pushed to Hugging Face.\n",
    "  - `hf_org`: The organization name on Hugging Face.\n",
    "  - `trainer_args`: Additional arguments for the trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hardcoded-config",
   "metadata": {},
   "source": [
    "## Hardcoded Configuration\n",
    "Here we hardcode the configuration settings directly into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hardcoded-config-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "  name: mistralai/Mistral-7B-Instruct-v0.1\n",
      "  new_model: Mistral-7B-Instruct-detcext-v0.1\n",
      "  quantization:\n",
      "    load_in_4bit: true\n",
      "    bnb_4bit_use_double_quant: true\n",
      "    bnb_4bit_quant_type: nf4\n",
      "    bnb_4bit_compute_dtype: bfloat16\n",
      "  device_map:\n",
      "    0: ''\n",
      "dataset:\n",
      "  path: ../data/eval/example_instruction_dataset.jsonl\n",
      "  type: alpaca\n",
      "  from_huggingface: false\n",
      "training:\n",
      "  output_dir: ./output\n",
      "  peft_enabled: true\n",
      "  peft_config:\n",
      "    r: 8\n",
      "    lora_alpha: 32\n",
      "    target_modules:\n",
      "    - q_proj\n",
      "    - v_proj\n",
      "    lora_dropout: 0.05\n",
      "    bias: none\n",
      "    task_type: CAUSAL_LM\n",
      "  hf_push: true\n",
      "  hf_org: my-organization\n",
      "  trainer_args:\n",
      "    per_device_train_batch_size: 1\n",
      "    gradient_accumulation_steps: 4\n",
      "    max_steps: 100\n",
      "    learning_rate: 0.0002\n",
      "    logging_steps: 1\n",
      "    save_strategy: epoch\n",
      "    optim: paged_adamw_8bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'model': {\n",
    "        'name': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "        'new_model': 'Mistral-7B-Instruct-detcext-v0.1',\n",
    "        'quantization': {\n",
    "            'load_in_4bit': True,\n",
    "            'bnb_4bit_use_double_quant': True,\n",
    "            'bnb_4bit_quant_type': 'nf4',\n",
    "            'bnb_4bit_compute_dtype': 'bfloat16'\n",
    "        },\n",
    "        'device_map': {\n",
    "            0: \"\"  # Correct device map\n",
    "        }\n",
    "    },\n",
    "    'dataset': {\n",
    "        'path': '../data/eval/example_instruction_dataset.jsonl',\n",
    "        'type': 'alpaca',\n",
    "        'from_huggingface': False\n",
    "    },\n",
    "    'training': {\n",
    "        'output_dir': './output',\n",
    "        'peft_enabled': True,\n",
    "        'peft_config': {\n",
    "            'r': 8,\n",
    "            'lora_alpha': 32,\n",
    "            'target_modules': ['q_proj', 'v_proj'],  # Update with correct target modules\n",
    "            'lora_dropout': 0.05,\n",
    "            'bias': 'none',\n",
    "            'task_type': 'CAUSAL_LM'\n",
    "        },\n",
    "        'hf_push': True,\n",
    "        'hf_org': 'my-organization',\n",
    "        'trainer_args': {\n",
    "            'per_device_train_batch_size': 1,\n",
    "            'gradient_accumulation_steps': 4,\n",
    "            'max_steps': 100,\n",
    "            'learning_rate': 0.0002,\n",
    "            'logging_steps': 1,\n",
    "            'save_strategy': 'epoch',\n",
    "            'optim': 'paged_adamw_8bit'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print the configuration to verify\n",
    "print(OmegaConf.to_yaml(OmegaConf.create(config)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-dataset",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Load the dataset using the `DataLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "load-dataset-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 8 examples [00:00, 90.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_loader = DataLoader(config['dataset']['path'], from_huggingface=config['dataset']['from_huggingface'])\n",
    "train_dataset, eval_dataset = data_loader.get_dataset()['train'].train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-model",
   "metadata": {},
   "source": [
    "## Setup Model and Tokenizer\n",
    "Setup the model and tokenizer with quantization and device configuration if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "setup-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model and tokenizer\n",
    "model_setup = ModelSetup(\n",
    "    config['model']['name'],\n",
    "    quantization_config=config['model']['quantization'],\n",
    "    device_map=config['model']['device_map']\n",
    ")\n",
    "model, tokenizer = model_setup.get_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-trainer",
   "metadata": {},
   "source": [
    "## Setup and Run Trainer\n",
    "Setup the `CustomTrainer` and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-trainer-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and run the trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    output_dir=config['training']['output_dir'],\n",
    "    peft_config=config['training']['peft_config'],  # Updated to use peft_config from config\n",
    "    **config['training']['trainer_args']\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-model",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Evaluate the model on the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluator = Evaluator(model, tokenizer, eval_dataset)\n",
    "results = evaluator.evaluate()\n",
    "print(\"Evaluation Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "push-model",
   "metadata": {},
   "source": [
    "## Push Model to Hugging Face\n",
    "If enabled, push the fine-tuned model to Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "push-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Hugging Face push is enabled\n",
    "if config['training']['hf_push']:\n",
    "    hf_integration = HuggingFaceIntegration(\n",
    "        model,\n",
    "        config['model']['name'],\n",
    "        config['model']['new_model'],\n",
    "        config['training']['hf_org']\n",
    "    )\n",
    "    hf_integration.save_and_push_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
